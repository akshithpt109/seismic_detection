{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshithpt109/seismic_detection/blob/main/Output_Catalogging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Extraction for Seismic Data Analysis\n",
        "\n",
        "This block of code is responsible for extracting various features from seismic waveform data stored in miniSEED files. Hereâ€™s an overview of the process:\n",
        "\n",
        "1. **Loading and Preprocessing Data**:\n",
        "   - `load_mseed()`: Loads the miniSEED file and extracts the trace data.\n",
        "   - `apply_bandpass_filter()`: Applies a bandpass filter to the trace to isolate the frequency range of interest.\n",
        "\n",
        "2. **Feature Computation**:\n",
        "   - `compute_sta_lta_with_time()`: Computes the Short-Term Average / Long-Term Average (STA/LTA) characteristic function for detecting seismic events and finds the time of the highest STA/LTA value.\n",
        "   - `detect_sta_lta_events()`: Detects start and end times of seismic events using STA/LTA values.\n",
        "   - `extract_sta_lta_features_with_time()`: Extracts additional STA/LTA-based features, including the maximum, mean, and variance of STA/LTA values.\n",
        "\n",
        "3. **Time Domain Features**:\n",
        "   - `extract_time_domain_features_with_max_min_time()`: Extracts features such as mean amplitude, maximum amplitude, second maximum amplitude, and RMS (Root Mean Square) amplitude. It also computes the times of these amplitude peaks.\n",
        "\n",
        "4. **Event Duration and Triggering Features**:\n",
        "   - `extract_event_duration()`: Computes the duration of seismic events based on STA/LTA triggers.\n",
        "   - `detect_zcr_threshold_in_batches()`, `detect_energy_threshold_in_batches()`, `detect_amplitude_spikes_in_batches()`: Detect time events based on zero-crossing rate (ZCR), cumulative energy thresholds, and amplitude spikes across different time windows.\n",
        "\n",
        "5. **Batch Processing**:\n",
        "   - `split_into_batches()`: Splits a trace into smaller batches to analyze events over specific time windows.\n",
        "   - These batches help in identifying features like energy peaks or ZCR exceedance within specific time frames.\n",
        "\n",
        "6. **Feature Extraction for Catalog**:\n",
        "   - `extract_features_for_catalog()`: Reads miniSEED files from a specified directory, processes each file to extract relevant features, and stores the results in a DataFrame. It combines features such as STA/LTA-based metrics, amplitude metrics, and event durations into a single dataset, which will be used for further analysis and model training.\n",
        "\n",
        "These features provide a detailed characterization of the seismic data and are used to train models that predict arrival times of seismic events in new data. By identifying key metrics like amplitude peaks, energy thresholds, and STA/LTA triggers, the model can better understand and predict seismic activity."
      ],
      "metadata": {
        "id": "dNaIKjVldueK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLb3NVhXdtF4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.fft import fft, fftfreq\n",
        "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
        "from obspy import read\n",
        "from datetime import datetime\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "#Load the miniSEED file\n",
        "def load_mseed(file_path):\n",
        "    st = read(file_path)\n",
        "    tr = st.traces[0].copy()\n",
        "    return tr\n",
        "\n",
        "#Apply a bandpass filter to the trace\n",
        "def apply_bandpass_filter(trace, min_freq, max_freq):\n",
        "    tr_filt = trace.copy()\n",
        "    tr_filt.filter('bandpass', freqmin=min_freq, freqmax=max_freq)\n",
        "    return tr_filt\n",
        "\n",
        "#Get the arrival time from the trace\n",
        "def get_arrival(tr, time):\n",
        "    starttime = tr.stats.starttime.datetime\n",
        "    return (time - starttime).total_seconds()\n",
        "\n",
        "#Function to compute STA/LTA with time\n",
        "def compute_sta_lta_with_time(trace, sta_len, lta_len):\n",
        "    sampling_rate = trace.stats.sampling_rate\n",
        "    cft = classic_sta_lta(trace.data, int(sta_len * sampling_rate), int(lta_len * sampling_rate))\n",
        "\n",
        "    max_sta_lta = np.max(cft)\n",
        "    max_sta_lta_idx = np.argmax(cft)\n",
        "\n",
        "\n",
        "    max_sta_lta_time = max_sta_lta_idx / sampling_rate\n",
        "\n",
        "    return cft, max_sta_lta, max_sta_lta_time\n",
        "\n",
        "#Function to detect STA/LTA events\n",
        "def detect_sta_lta_events(trace, sta_len, lta_len):\n",
        "    thr_on = 2.0\n",
        "    thr_off = 1.0\n",
        "    cft, _, _ = compute_sta_lta_with_time(trace, sta_len, lta_len)\n",
        "    on_off = trigger_onset(cft, thr_on, thr_off)\n",
        "\n",
        "    if len(on_off) > 0:\n",
        "        start_idx, end_idx = on_off[0]\n",
        "        start_time = start_idx / trace.stats.sampling_rate\n",
        "        end_time = end_idx / trace.stats.sampling_rate\n",
        "        return start_time, end_time\n",
        "    return None, None\n",
        "\n",
        "#Split trace into 120-second batches\n",
        "def split_into_batches(trace, batch_duration_sec=120):\n",
        "    sampling_rate = trace.stats.sampling_rate\n",
        "    batch_size = int(batch_duration_sec * sampling_rate)\n",
        "    total_length = len(trace.data)\n",
        "\n",
        "    #Create a list of batches\n",
        "    batches = []\n",
        "    for i in range(0, total_length, batch_size):\n",
        "        batch_data = trace.data[i:i + batch_size]\n",
        "        if len(batch_data) == batch_size:\n",
        "            batch_trace = trace.copy()\n",
        "            batch_trace.data = batch_data\n",
        "            batch_trace.stats.starttime += i / sampling_rate\n",
        "            batches.append(batch_trace)\n",
        "    return batches\n",
        "\n",
        "#Function to detect Zero-Crossing Rate (ZCR) threshold events in batches\n",
        "def detect_zcr_threshold_in_batches(trace, zcr_threshold):\n",
        "    batches = split_into_batches(trace)\n",
        "    for batch in batches:\n",
        "        zcr_exceed_time = detect_zcr_threshold_events(batch, zcr_threshold)\n",
        "        if zcr_exceed_time is not None:\n",
        "            return (batch.stats.starttime - trace.stats.starttime) + zcr_exceed_time\n",
        "    return None\n",
        "\n",
        "#Function to detect ZCR threshold events (individual batch)\n",
        "def detect_zcr_threshold_events(trace, zcr_threshold):\n",
        "    data = trace.data\n",
        "    sampling_rate = trace.stats.sampling_rate\n",
        "    zero_crossings = np.where(np.diff(np.sign(data)))[0]\n",
        "    zcr = len(zero_crossings) / len(data)\n",
        "\n",
        "    if zcr > zcr_threshold and len(zero_crossings) > 0:\n",
        "        zcr_exceed_time = zero_crossings[0] / sampling_rate\n",
        "        return zcr_exceed_time\n",
        "    return None\n",
        "\n",
        "#Function to detect when the cumulative energy exceeds a threshold in batches\n",
        "def detect_energy_threshold_in_batches(trace, energy_threshold):\n",
        "    batches = split_into_batches(trace)\n",
        "    for batch in batches:\n",
        "        energy_exceed_time = detect_energy_threshold_events(batch, energy_threshold)\n",
        "        if energy_exceed_time is not None:\n",
        "            return (batch.stats.starttime - trace.stats.starttime) + energy_exceed_time\n",
        "    return None\n",
        "\n",
        "#Function to detect energy threshold events (individual batch)\n",
        "def detect_energy_threshold_events(trace, energy_threshold):\n",
        "    data = trace.data\n",
        "    cumulative_energy = np.cumsum(data**2) / np.max(np.cumsum(data**2))\n",
        "\n",
        "    exceed_indices = np.where(cumulative_energy > energy_threshold)[0]\n",
        "\n",
        "    if len(exceed_indices) > 0:\n",
        "        energy_exceed_time = exceed_indices[0] / trace.stats.sampling_rate\n",
        "        return energy_exceed_time\n",
        "    return None\n",
        "\n",
        "#Function to detect amplitude spikes in batches\n",
        "def detect_amplitude_spikes_in_batches(trace, amp_threshold):\n",
        "    batches = split_into_batches(trace)\n",
        "    for batch in batches:\n",
        "        spike_time = detect_amplitude_spikes(batch, amp_threshold)\n",
        "        if spike_time is not None:\n",
        "            return (batch.stats.starttime - trace.stats.starttime) + spike_time\n",
        "    return None\n",
        "\n",
        "#Function to detect amplitude spikes (individual batch)\n",
        "def detect_amplitude_spikes(trace, amp_threshold):\n",
        "    data = trace.data\n",
        "    sampling_rate = trace.stats.sampling_rate\n",
        "    spike_indices = np.where(np.abs(data) > amp_threshold)[0]\n",
        "\n",
        "    if len(spike_indices) > 0:\n",
        "        spike_time = spike_indices[0] / sampling_rate\n",
        "        return spike_time\n",
        "    return None\n",
        "\n",
        "def highest_average_amplitude(mseed_file, batch_duration_sec=180):\n",
        "    st = read(mseed_file)\n",
        "    trace = st.traces[0].copy()\n",
        "    sampling_rate = trace.stats.sampling_rate\n",
        "    batch_size = int(batch_duration_sec * sampling_rate)\n",
        "    total_length = len(trace.data)\n",
        "\n",
        "    max_average_amplitude = None\n",
        "    time_of_max_amplitude = None\n",
        "\n",
        "    for i in range(0, total_length, batch_size):\n",
        "        batch_data = trace.data[i:i + batch_size]\n",
        "        if len(batch_data) == batch_size:\n",
        "            avg_amplitude = np.mean(np.abs(batch_data))\n",
        "            if max_average_amplitude is None or avg_amplitude > max_average_amplitude:\n",
        "                max_average_amplitude = avg_amplitude\n",
        "                time_of_max_amplitude = i / sampling_rate\n",
        "    return max_average_amplitude, time_of_max_amplitude\n",
        "\n",
        "#Function to extract STA/LTA features with time\n",
        "def extract_sta_lta_features_with_time(trace, sta_len, lta_len):\n",
        "    cft, max_sta_lta, max_sta_lta_time = compute_sta_lta_with_time(trace, sta_len, lta_len)\n",
        "    mean_sta_lta = np.mean(cft)\n",
        "    var_sta_lta = np.var(cft)\n",
        "    return max_sta_lta, max_sta_lta_time, mean_sta_lta, var_sta_lta\n",
        "\n",
        "#Function to extract amplitude features with min and max times\n",
        "def extract_time_domain_features_with_max_min_time(trace):\n",
        "    data = trace.data\n",
        "    sampling_rate = trace.stats.sampling_rate\n",
        "    mean_amp = np.mean(data)\n",
        "    rms_amp = np.sqrt(np.mean(data**2))\n",
        "    sorted_amplitudes = np.argsort(data)\n",
        "    max_amp_idx = sorted_amplitudes[-1]\n",
        "    second_max_amp_idx = sorted_amplitudes[-2]\n",
        "    max_amp = data[max_amp_idx]\n",
        "    second_max_amp = data[second_max_amp_idx]\n",
        "    min_amp = np.min(data)\n",
        "    min_amp_idx = np.argmin(data)\n",
        "    max_amp_time = max_amp_idx / sampling_rate\n",
        "    second_max_amp_time = second_max_amp_idx / sampling_rate\n",
        "    min_amp_time = min_amp_idx / sampling_rate\n",
        "    return mean_amp, max_amp, max_amp_time, second_max_amp, second_max_amp_time, min_amp, min_amp_time, rms_amp\n",
        "\n",
        "\n",
        "\n",
        "#Function to extract event duration features\n",
        "def extract_event_duration(trace, sta_len, lta_len):\n",
        "    thr_on = 4.0\n",
        "    thr_off = 1.5\n",
        "    cft = compute_sta_lta_with_time(trace, sta_len, lta_len)[0]\n",
        "    on_off = trigger_onset(cft, thr_on, thr_off)\n",
        "    if len(on_off) > 0:\n",
        "        event_durations = [(end - start) / trace.stats.sampling_rate for start, end in on_off]\n",
        "        return np.mean(event_durations) if event_durations else 0\n",
        "    return 0\n",
        "\n",
        "#Main function to extract features from the catalog\n",
        "def extract_features_for_catalog(cat_file_path, data_directory):\n",
        "\n",
        "    features_list = []\n",
        "\n",
        "    try:\n",
        "        filename = data_directory\n",
        "        mseed_file = f'{cat_file_path}.mseed'\n",
        "\n",
        "        if load_mseed(mseed_file):\n",
        "            tr = load_mseed(mseed_file)\n",
        "            tr_filt = apply_bandpass_filter(tr, 0.5, 1.0)\n",
        "            max_sta_lta, max_sta_lta_time, mean_sta_lta, var_sta_lta = extract_sta_lta_features_with_time(tr_filt, 0.5, 10)\n",
        "            mean_amp, max_amp, max_amp_time, second_max_amp, second_max_amp_time, min_amp, min_amp_time, rms_amp = extract_time_domain_features_with_max_min_time(tr)\n",
        "\n",
        "            zcr_threshold_time = detect_zcr_threshold_in_batches(tr_filt, zcr_threshold=0.1)\n",
        "            energy_threshold_time = detect_energy_threshold_in_batches(tr_filt, energy_threshold=0.7)\n",
        "            amp_spike_time = detect_amplitude_spikes_in_batches(tr_filt, amp_threshold=0.4)\n",
        "            highest_avg_amplitude1, time_of_occurrence1 = highest_average_amplitude(mseed_file, batch_duration_sec=200)\n",
        "            highest_avg_amplitude2, time_of_occurrence2 = highest_average_amplitude(mseed_file, batch_duration_sec=475)\n",
        "\n",
        "\n",
        "                #STA/LTA event detection\n",
        "            sta_lta_start_time, sta_lta_end_time = detect_sta_lta_events(tr_filt, sta_len=200, lta_len=475)\n",
        "\n",
        "\n",
        "            features_list.append({\n",
        "                'filename': filename,\n",
        "                'max_sta_lta': max_sta_lta,\n",
        "                'max_sta_lta_time': max_sta_lta_time,\n",
        "                'mean_sta_lta': mean_sta_lta,\n",
        "                'var_sta_lta': var_sta_lta,\n",
        "                'mean_amp': mean_amp,\n",
        "                'max_amp': max_amp,\n",
        "                'max_amp_time': max_amp_time,\n",
        "                'second_max_amp': second_max_amp,\n",
        "                'second_max_amp_time': second_max_amp_time,\n",
        "                'min_amp': min_amp,\n",
        "                'min_amp_time': min_amp_time,\n",
        "                'rms_amp': rms_amp,\n",
        "                'zcr_threshold_time': zcr_threshold_time,\n",
        "                'energy_threshold_time': energy_threshold_time,\n",
        "                'amp_spike_time': amp_spike_time,\n",
        "                'highest_avg_amplitude1': highest_avg_amplitude1,\n",
        "                'time_of_occurrence1': time_of_occurrence1,\n",
        "                'highest_avg_amplitude2': highest_avg_amplitude2,\n",
        "                'time_of_occurrence2': time_of_occurrence2,\n",
        "                'sta_lta_start_time': sta_lta_start_time,\n",
        "                'sta_lta_end_time': sta_lta_end_time\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'Error processing row {i}: {e}')\n",
        "\n",
        "    return pd.DataFrame(features_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting Arrival Times Using KNN Model\n",
        "\n",
        "This block of code is focused on training a K-Nearest Neighbors (KNN) model to predict the arrival times of seismic events based on extracted features from the miniSEED files. The steps involved are as follows:\n",
        "\n",
        "1. **Load and Preprocess the Training Data**:\n",
        "   - Reads the training dataset from `Features.csv` and drops any columns with missing values.\n",
        "   - Selects relevant features and scales them using `StandardScaler` to ensure that all features have zero mean and unit variance.\n",
        "   - Defines the target variable `y_absolute` as the 'arrival' time.\n",
        "   - Splits the data into training and testing sets for model evaluation.\n",
        "\n",
        "2. **Feature Extraction for Testing Data**:\n",
        "   - Iterates over a directory containing the test miniSEED files, extracting features using the `extract_features_for_catalog()` function for each file.\n",
        "   - Concatenates all the extracted features into a single DataFrame, preparing them for model prediction.\n",
        "\n",
        "3. **Prepare Test Data for Prediction**:\n",
        "   - Prepares the test features by dropping columns not used for prediction and scales them using the same scaler fitted on the training data.\n",
        "   - Ensures that the feature space matches between training and test data to avoid errors during prediction.\n",
        "\n",
        "4. **Train and Apply the KNN Model**:\n",
        "   - Trains a KNN model with `n_neighbors=5` using the training dataset (`X_train_abs` and `y_train_abs`).\n",
        "   - Uses the trained model to predict arrival times for the test dataset.\n",
        "\n",
        "5. **Store and Save Predictions**:\n",
        "   - Stores the predicted arrival times along with their corresponding filenames into a list of dictionaries.\n",
        "   - Converts this list into a DataFrame called `output_df`, which contains the filenames and their respective predicted arrival times (`time_rel`).\n",
        "   - Saves the predictions to a CSV file (`output_catalog.csv`) for further analysis or reporting."
      ],
      "metadata": {
        "id": "kRq-XSFXfV5w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4Z4L77UdtF6",
        "outputId": "036877f4-276a-4605-afbe-16b31c5eb885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                 filename    time_rel\n",
            "0  xa.s12.00.mhz.1969-12-16HR00_evid00006   7583.5416\n",
            "1  xa.s12.00.mhz.1970-01-09HR00_evid00007  55019.5094\n",
            "2  xa.s12.00.mhz.1970-02-07HR00_evid00014  68063.6764\n",
            "3  xa.s12.00.mhz.1970-02-18HR00_evid00016  51983.6214\n",
            "4  xa.s12.00.mhz.1970-03-14HR00_evid00018  36275.6548\n",
            "Predictions saved to output_catalog.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\akshi\\Machine Learning Projects\\Space Apps Challenge\\venv\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Load the initial dataset and preprocess\n",
        "df = pd.read_csv(\"Features.csv\")\n",
        "df = df.dropna(axis=1)\n",
        "features_df_clean = df.drop(columns=['arrival','zcr_threshold_time', 'sta_lta_start_time', 'max_sta_lta', 'mean_sta_lta', 'var_sta_lta', 'mean_amp', 'max_amp', 'second_max_amp', 'min_amp', 'rms_amp', 'highest_avg_amplitude1', 'highest_avg_amplitude2'])\n",
        "scaler = StandardScaler()\n",
        "features_df_scaled = scaler.fit_transform(features_df_clean)\n",
        "X = features_df_scaled\n",
        "y_absolute = df['arrival']\n",
        "X_train_abs, X_test_abs, y_train_abs, y_test_abs = train_test_split(X, y_absolute, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "data_dir = r'C:\\Users\\akshi\\Machine Learning Projects\\Space Apps Challenge\\data\\lunar\\test\\data'\n",
        "\n",
        "\n",
        "all_features = []\n",
        "\n",
        "\n",
        "for root, dirs, files in os.walk(data_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.mseed'):\n",
        "\n",
        "            filepath_without_extension = os.path.join(root, os.path.splitext(file)[0])\n",
        "\n",
        "\n",
        "            features_df = extract_features_for_catalog(filepath_without_extension, os.path.splitext(file)[0])\n",
        "\n",
        "\n",
        "            all_features.append(features_df)\n",
        "\n",
        "#Concatenate all extracted feature data into a single DataFrame\n",
        "final_dataset = pd.concat(all_features, ignore_index=True)\n",
        "X_test = final_dataset.drop(columns=['filename', 'zcr_threshold_time', 'sta_lta_start_time', 'max_sta_lta', 'mean_sta_lta', 'var_sta_lta', 'mean_amp', 'max_amp', 'second_max_amp', 'min_amp', 'rms_amp', 'highest_avg_amplitude1', 'highest_avg_amplitude2']).dropna(axis=1).values\n",
        "\n",
        "#Scale the features using the same scaler as the training data\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "#Train the KNN model (ensure the model is properly trained)\n",
        "knn_model = KNeighborsRegressor(n_neighbors=5)\n",
        "knn_model.fit(X_train_abs, y_train_abs)\n",
        "\n",
        "\n",
        "predictions = knn_model.predict(X_test_scaled)\n",
        "\n",
        "\n",
        "results = []\n",
        "for i, pred in enumerate(predictions):\n",
        "    results.append({\n",
        "        'filename': final_dataset['filename'].iloc[i],\n",
        "        'time_rel': pred\n",
        "    })\n",
        "\n",
        "\n",
        "output_df = pd.DataFrame(results)\n",
        "output_file = 'output_catalog.csv'\n",
        "output_df.to_csv(output_file, index=False)\n",
        "print(f\"Predictions saved to {output_file}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}